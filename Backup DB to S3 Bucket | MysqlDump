
###########################################################
# Backup DB to S3 Bucket | MysqlDump
###########
# Considerations:
# 0. Make the Backups from a clone to avoid locking DB during backup upload period.
# 1. Mount second disk to server, extend logical volume to accomodate for backup
# 2. Or expand current root
#######
###

#!/usr/bin/bash

sudo cat << EOF
>> Script to Backup && Restore DB data to/from Local Dir/S3 Mounted FileSystem <<

>> [Caution]: Ensure enough storage available on local FS !!
>> Root Storage: $( df -hT | grep -i ext4 | head -1)
>> OS : $( cat /etc/os-release | grep -i PRETTY | awk -F "=" '{print$2}')

>> STEPS:
>> 0. Escalate to Root User
>> 1. Installs tool to mount s3 to filesytem -- s3fs
>> 2. Install & Configure AWS CLI & Create credentials -- aws configure
>> 3. Mount S3 to fs
>> 4. DB backup Raw/Zipped to Local Dir || to S3 Bucket
>> 5. DB restore Raw/Zipped to Local Dir from S3 Bucket

>> [+] Starting backups...
EOF

echo "[+] Enter User Input Options:"
read -p 'DB_USER: ' DB_USER
read -p 'DB_PASS: ' DB_PASS
read -p 'DB_URL: ' DB_URL
read -p 'DB_PORT: ' DB_PORT
read -p 'DB_NAME: ' DB_NAME
read -p 'S3_NAME: ' S3_NAME
read -p 'S3_REGION: ' S3_REGION
read -p 'Access_Key: ' accesskey
read -p 'Secret_Key: ' secretkey

#Install s3fs -- tool to mount S3 to Filesystem
echo "[+] Install s3fs tool..."
sudo apt-get install s3fs -y

echo "[+] Create S3 mountpoint if none-exits/ignore If exits..."
#Create S3 mountpoint if none existent
sudo mkdir -p /mnt/s3

#Install AWS CLI
sudo echo "[+] Installing AWS CLI..."
sudo curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
sudo apt install unzip -y
sudo unzip awscliv2.zip
sudo ./aws/install
sudo aws configure

# sync local_dir with S3 bucket
echo "[+] Sync local_dir to S3 Bucket..."
sudo aws s3 sync /mnt/s3 s3://$S3_NAME

# create credentials for s3fs
echo "[+] Create credentials for s3fs..."
sudo echo $accesskey:$secretkey > ${HOME}/.passwd-s3fs
sudo chmod 600 ${HOME}/.passwd-s3fs

# Mount the S3 bucket to FS
sudo s3fs $S3_NAME /mnt/s3  -o passwd_file=/root/.passwd-s3fs,nonempty,rw,allow_other,mp_umask=002,uid=1000,gid=1000 -o url=http://s3.$S3_REGION.amazonaws.com

# persist mountpoint
echo $S3_NAME /mnt/s3 fuse.s3fs _netdev,allow_other 0 0 >> /etc/fstab

echo "[+] S3 Mount Sync: $(df -hT | grep -i s3fs)"
echo "[+] >> Backup DB With -->> mysqldump << "
echo "[+] -----------EXAMPLE----------"
echo "[+]  ./backupdb.sh <option>"
echo "[+] ----------------------------"
echo "[+] ...OPTIONS TO SELECT..."
echo "[+] --localzip || -lz  --  ZIP && DB dump to Local Dir(/curr_directory/***)"
echo "[+] --s3zip || -s3z  --  ZIP && Backup DB dump to S3 mountpoint(/mnt/s3/***)"
echo "[+] --s3raw || -s3r  --  Backup RAW DB dump to S3 myountpoint(/mnt/s3/***)"
echo "[+] --copyraw2local || -cr2l -- Copy Raw Dump from S3 to Local Current Dir"
echo "[+] --copyzip2local || -cz2l -- Copy Raw Dump from S3 to Local Current Dir"

read -p "Option: " Option

echo "[*] Start Time: $(date)" >> output.log
echo "[*] ***************************" >> output.log

if [[  $Option == "--localzip" || $Option == "-lz" ]]
then
        #[Faster Recovery from local FileSystem] - ZIP && DB dump to Local Dir
        sudo mysqldump --no-tablespaces -h $DB_URL -u $DB_USER -p$DB_PASS $DB_NAME | gzip >> $(pwd)/BACKUPS/$(date -I)_$DB_NAME-db_dump.sql.gz
elif [[ $Option == "--s3zip" || $Option == "-s3z" ]]
then
        #[Least Space used] - ZIP && Backup DB and store in S3 Mountpoint
        sudo mysqldump --no-tablespaces -h $DB_URL -u $DB_USER -p$DB_PASS $DB_NAME | gzip >>  /mnt/s3/$(date -I)_$DB_NAME-db_dump.sql.gz
elif [[ $Option == "--s3raw" || $Option == "-s3r" ]]
then
        #[Easy Recovery] - Backup _RAW_ DB and store in S3 Mountpoint
        sudo mysqldump --no-tablespaces -h $DB_URL -u $DB_USER -p$DB_PASS $DB_NAME >>  /mnt/s3/$(date -I)_$DB_NAME-db_dump.sql

##-->> Copy DB Dump Files Back to Local Dir

elif [[ $Option == "--copyraw2local" || $Option == "-cr2l" ]]
then
        #[Copy Raw Backups] - From S3 to Local Current Dir
        read -p "Date:(format >> YYYY-MM-DD )" DATE
        read -p "File Store Location:(foramt >> /tmp/)" LOCATION
        sudo aws cp s3://$S3_NAME/$DATE_$DB_NAME-db_dump.sql $LOCATION

elif [[ $Option == "--copyzip2local" || $Option == "-cz2l" ]]
then
        #[Copy Zip Backups] - From S3 to Local Current Dir and Decompress to CWD
        read -p "Date:(format >> YYYY-MM-DD ): " DATE
        read -p "File Store Location:(foramt >> /opt/BACKUPS): " LOCATION
        sudo aws cp s3://$S3_NAME/$DATE_$DB_NAME-db_dump.sql.gz $LOCATION
        # sudo gunzip -dvk $DATE_$DB_NAME-db_dump.sql.gz
        # gunzip -dvk **-**-**-db_dump.sql.gz

else
        echo "[+] ** Wrong OR No options Selected**"
fi
echo "[*] $(ls -lsah /mnt/s3/)" >> output.log
echo "[*] ***************************" >> output.log
echo "[*] End Time: $(date)" >> output.log
