## LOG DUMPS TO S3
## USAGE
-----------------------------------------------------------------
>> The scripts iterates over a directory and uploads all files ending in ".log" after which clears the files.
>> Use below steps to upload log files to an s3 bucket and clear storage

## PRE-REQUISITES
-------------------------------------------------------------------
>> Ensure the user has adequate user rights to the specified S3 bucket

## STEP 1: INSTALL AWS CLI
-------------------------------------------------------------------
$ cd /opt
$ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
$ unzip awscliv2.zip
$ sudo ./aws/install

## STEP 2: CONFIGURE USER PROFILE
--------------------------------
>> (use sudo prefarably to be able to access privileged logs) <<
$ sudo aws configure --profile <<backupuser>>
AWS Access Key ID [****************]: xxxxxx
AWS Secret Access Key [****************]: xxxxx
Default region name []: eu-west-1 
Default output format []: json

## STEP 3: USE BELOW SCRIPT(backup_v2.sh) TO PUSH LOGS TO S3 ==> 
## chmod +x backup_v2.sh
--------------------------------------------------------------------
#!/bin/bash

if [[ $1 ]]
then
for filename in $1*.log;
        do
        aws s3 cp \
        $filename s3://<<bucket-name>>/<<AWS|NTNX>>/logs/$(date -I)/$(curl ifconfig.me)/$(basename $filename) \
        --profile <<backupuser>> \
        >> status.log 
	echo "" > $filename ;done
else
        echo "**** INCLUDE LOG PATH LOCATION ****"
        echo "###################################"
        echo " ./backup_v2.sh /path/location/log/"
        echo "###################################"
fi

### prefix location of logs either AWS | NTNX
>> <<bucket_name>> - name of bucket
>> <<AWS | NTNX>>  - server host location
>> <<backupuser>>  - IAM User set for profile
 
## STEP 4: RUN SCRIPT (manually OR cronjob)
--------------------------------------------------------------------
./backup.sh /path/to/logs/
